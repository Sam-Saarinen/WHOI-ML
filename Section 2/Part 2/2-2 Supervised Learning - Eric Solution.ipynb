{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting Classification as Regression, Regressing to Probabilities\n",
    "1. We can turn classification labels into a one-hot vector.\n",
    "2. We can regress to the vector.\n",
    "3. To produce output classes, we can take the element with highest weight.\n",
    "4. The regressed value can be interpreted as an (approximate) probability.\n",
    "\n",
    "Regressing to probabilities is a useful trick, especially when we start thinking about confidences and unsupervised data analysis.\n",
    "\n",
    "[Link to Fish Dataset Details](https://www.kaggle.com/aungpyaeap/fish-market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "['\\ufeffSpecies', 'Weight', 'Length1', 'Length2', 'Length3', 'Height', 'Width']\n",
      "['Bream', '242', '23.2', '25.4', '30', '11.52', '4.02']\n",
      "{'Roach': 1, 'Parkki': 3, 'Whitefish': 2, 'Pike': 5, 'Perch': 4, 'Smelt': 6, 'Bream': 0}\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open('Fish.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        rows.append(row)\n",
    "\n",
    "print(len(rows))\n",
    "print(rows[0]) # first row is a header\n",
    "print(rows[1])\n",
    "\n",
    "rows = rows[1:]\n",
    "\n",
    "labels = {} # Create a dictionary of label strings to numeric values\n",
    "for row in rows:\n",
    "    if row[0] not in labels:\n",
    "        labels[row[0]]=len(labels)\n",
    "\n",
    "print(labels)\n",
    "        \n",
    "inputs = np.array([[float(c) for c in (row[1:])] for row in rows])\n",
    "outputs = np.array([labels[row[0]] for row in rows])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def output_to_one_hot(categories, max_val):\n",
    "    data = np.zeros((len(categories), max_val))\n",
    "    data[np.arange(len(categories)), categories] = 1\n",
    "    return data\n",
    "\n",
    "encodings = output_to_one_hot(outputs, len(labels))\n",
    "print(encodings[:10])\n",
    "print(encodings[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "1. Define a network class that regresses to the 7 outputs.\n",
    "2. Train a sufficiently large network to perform the categorization.\n",
    "3. Measure the test accuracy of the model by counting the number of accurate labels\n",
    "\n",
    "# Stretch Goals:\n",
    "- Test out different network architectures (depth, breadth) and examine training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, breadth=500, depth=3):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        hidden_layers = depth - 2\n",
    "        self.fcs = [nn.Linear(6, breadth)]\n",
    "        self.fcs.extend([nn.Linear(breadth, breadth)]*hidden_layers)\n",
    "        \n",
    "        for i in np.arange(len(self.fcs)):\n",
    "            self.add_module('fc{}'.format(i), self.fcs[i])\n",
    "        \n",
    "        self.final_fc = nn.Linear(breadth, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc in self.fcs:\n",
    "            x = F.relu(fc(x))\n",
    "        x = self.final_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, inputs, true_vals, num_epochs=1000):\n",
    "        inputs = torch.from_numpy(inputs).float()\n",
    "        true_vals = torch.from_numpy(true_vals).float()\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        net = self\n",
    "        net.float() # force float type\n",
    "        net.zero_grad()\n",
    "        \n",
    "        outputs = net(Variable(torch.Tensor([0]*6)))\n",
    "        outputs.backward(torch.randn(7)) # Use random gradients to break symmetry?\n",
    "\n",
    "        learning_rate = 1 # Need to initialize carefully\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "        # create your optimizer\n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        t = trange(num_epochs)\n",
    "        for epoch in t:  # loop over the dataset multiple times\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # wrap them in Variable\n",
    "            reshaped_inputs = inputs #.view(-1, 1) # Structure with each input in its own row\n",
    "            reshaped_outputs = true_vals #.view(-1, 1) # Neglecting to have outputs and true vals to match dimension is a common mistake.\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(reshaped_inputs)\n",
    "            #print(outputs)\n",
    "            #print(reshaped_outputs)\n",
    "            #loss = criterion(outputs, reshaped_outputs)\n",
    "            error = reshaped_outputs - outputs\n",
    "            #print(\"ERROR\")\n",
    "            #print(error)\n",
    "            loss = (error ** 2).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            t.set_description('ML: loss={}'.format(loss.item())) # Updates Loss information\n",
    "\n",
    "        t1 = time.time() - t0\n",
    "\n",
    "        return loss, t1\n",
    "    \n",
    "    def categorize(self, inputs):\n",
    "        inputs = torch.from_numpy(inputs).float()\n",
    "        outputs = net(inputs).detach().numpy()\n",
    "        \n",
    "        # make hard decisions\n",
    "        hard_output = np.zeros_like(outputs)\n",
    "        hard_output[np.arange(len(outputs)), outputs.argmax(1)] = 1\n",
    "        return hard_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc0): Linear(in_features=6, out_features=500, bias=True)\n",
      "  (fc1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (final_fc): Linear(in_features=500, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.08434455841779709: 100%|██████████| 1000/1000 [00:05<00:00, 193.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0843, grad_fn=<MeanBackward0>), 5.19073748588562)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y_categorized_test = net.categorize(X_test)\n",
    "\n",
    "# The number of accurate labels\n",
    "num_accurate_labels = np.sum(y_categorized_test == y_test) / y_test.shape[1]\n",
    "\n",
    "frac_correct = num_accurate_labels / len(y_test)\n",
    "print(frac_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.285714285714285"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_accurate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.06734256446361542: 100%|██████████| 1000/1000 [00:04<00:00, 244.74it/s]\n",
      "ML: loss=15.752217292785645:   2%|▏         | 21/1000 [00:00<00:09, 107.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 250, Depth: 3, 0.8571428571428571 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.09997686743736267: 100%|██████████| 1000/1000 [00:04<00:00, 202.98it/s]\n",
      "ML: loss=0.9487798810005188:   2%|▏         | 18/1000 [00:00<00:10, 91.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 250, Depth: 4, 0.8142857142857143 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.05704564228653908: 100%|██████████| 1000/1000 [00:05<00:00, 174.90it/s]\n",
      "ML: loss=3572.078369140625:   2%|▏         | 19/1000 [00:00<00:09, 99.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 250, Depth: 5, 0.8785714285714287 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.1714099794626236: 100%|██████████| 1000/1000 [00:05<00:00, 195.34it/s]\n",
      "ML: loss=31.32442283630371:   2%|▏         | 15/1000 [00:00<00:12, 77.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 500, Depth: 3, 0.8428571428571429 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.07283200323581696: 100%|██████████| 1000/1000 [00:07<00:00, 140.79it/s]\n",
      "ML: loss=0.670676052570343:   1%|          | 11/1000 [00:00<00:17, 57.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 500, Depth: 4, 0.8714285714285713 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.1396862417459488: 100%|██████████| 1000/1000 [00:09<00:00, 110.12it/s]\n",
      "ML: loss=520990.03125:   1%|          | 8/1000 [00:00<00:22, 43.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 500, Depth: 5, 0.8642857142857142 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.3549141585826874: 100%|██████████| 1000/1000 [00:12<00:00, 82.69it/s]\n",
      "ML: loss=364.1068115234375:   1%|          | 6/1000 [00:00<00:29, 33.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 1000, Depth: 3, 0.8357142857142857 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.09415100514888763: 100%|██████████| 1000/1000 [00:20<00:00, 49.03it/s]\n",
      "ML: loss=7.513808250427246:   0%|          | 4/1000 [00:00<00:44, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 1000, Depth: 4, 0.8571428571428571 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ML: loss=0.07752490043640137: 100%|██████████| 1000/1000 [00:25<00:00, 39.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breadth: 1000, Depth: 5, 0.85 correct\n"
     ]
    }
   ],
   "source": [
    "# Test different parameters\n",
    "breadths = [250, 500, 1000]\n",
    "depths = [3, 4, 5]\n",
    "\n",
    "frac_corrects = np.zeros([len(breadths), len(depths)])\n",
    "\n",
    "for b in np.arange(len(breadths)):\n",
    "        breadth = breadths[b]\n",
    "\n",
    "        for d in np.arange(len(depths)):\n",
    "            depth = depths[d]\n",
    "            \n",
    "            net = Net(breadth=breadth, depth=depth)\n",
    "            net.train(X_train, y_train)\n",
    "            \n",
    "            y_categorized_test = net.categorize(X_test)\n",
    "            # The number of accurate labels\n",
    "            num_accurate_labels = np.sum(y_categorized_test == y_test) / y_test.shape[1]\n",
    "\n",
    "            frac_correct = num_accurate_labels / len(y_test)\n",
    "            print(\"Breadth: {}, Depth: {}, {} correct\".format(breadth, depth, frac_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
