{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Pattern: Measure Loss vs. Time and Number of Samples\n",
    "\n",
    "Now that we have a basis for comparing models for regression, why not just try a bunch and pick the best one? In fact, this will be an approach that we take from now on, whenever we have multiple models available to us. In this part, we're going to design some experimental protocols that will let us formally compare different models. This will also give us practice using plots of performance as a tool for understanding individual algorithms.\n",
    "\n",
    "While we could just take a single measure of performance and be done, this may not give us the whole story on an algorithm. Generally, we care about two quantities: _data efficiency_ and _time efficiency_. Good data efficiency means that a model performs well with as few datapoints as possible. As a counter-example, I might be able to predict someone's age from their face if I met literally every person in the world, but that clearly isn't that useful. Good time efficiency means that the model achieves good performance on a fixed set of data quickly. Many models introduce trade-offs between time efficiency and data efficiency.\n",
    "\n",
    "# Boosting\n",
    "# Random Forests\n",
    "# K-NN\n",
    "\n",
    "https://github.com/tqdm/tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 98.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "data = np.loadtxt(\"test_data.csv\", delimiter=\",\")\n",
    "#print(data)\n",
    "\n",
    "Xs = data[:,0:1]\n",
    "Ys = data[:,2]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "t = trange(100)\n",
    "for i in t:\n",
    "    model = GradientBoostingRegressor(n_estimators=i+1)\n",
    "    model.fit(Xs, Ys)\n",
    "    predictions = model.predict(Xs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "1. Plot performance vs. time for AdaBoost (using random forests). Pick a cutoff number of models.\n",
    "2. Plot Performance vs. Number of Input Points for K-NN and AdaBoost.\n",
    "3. Plot Time vs. Number of Input Points for K-NN, Adaboost, and Ridge Regression.\n",
    "\n",
    "# Stretch Goals:\n",
    "- Run the tests multiple times and compute means and standard deviations. Plot 95% confidence regions around each curve.\n",
    "- Run the experiment with other regression models from sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
