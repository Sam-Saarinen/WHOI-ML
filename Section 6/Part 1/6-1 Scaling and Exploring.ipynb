{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is all assuming that you have already looked at ways of improving learning by adjusting network architecture and parameters. Your first goal should always be to overfit on a small training set before scaling to the full problem.\n",
    "\n",
    "# Parallelization - GPU's\n",
    "GPU's are good at multiplying matrices quickly in parallel. Layered Neural Networks perform maxtrix operations to scale inputs by appropriate weights, and thus can benefit from GPU hardware. NVidia GPU's have a library called CUDA that pytorch can use to perform neural network learning.\n",
    "\n",
    "# Parallelization - multi-GPU's for memory constraints\n",
    "Evaluating and using the neural network requires having our training inputs in memory. For large datasets and inputs, the time to load data into the GPU memory can dominate the cost of inference. For very large neural networks (tens of millions of parameters), storing the network weights themselves can also be a significant drain. Parallel GPU's can compute gradients over different portions of the training set, combine the gradient estimates, update the weights centrally, and propagate the updated networks. (Alternately, they can represent different portions of the same network in a computational pipeline.)\n",
    "\n",
    "# Parallelization - Multi-processors with weight sharing for less time\n",
    "In addition to parallelization to reduce the cost of storage in memory, networks can be parallelized across processors (with or without shared memory - for example, multi-core systems) to reduce the amount of time to accumulate gradients across the entire batch.\n",
    "\n",
    "# Parallelization - To the extreme\n",
    "In the limit, a large number of networked GPUs can each process a small handful of examples, send their gradients to a master weight-updater, and receive the most recent network weights in return. This separates and parallelizes the data processing from the optimization step, which could happen asynchronously (e.g., every time at least n machines return gradients, run the optimizer on their mean).\n",
    "\n",
    "# Other directions\n",
    "It may also be possible to make headway by considering sub-problems that could be composed. For example, there may be a clean way to divide up the input space into contiguous regions and learn smaller models local to each sub-problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
