{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Into the Great Unknown...\n",
    "So far, we've learned about a wealth of machine learning tools for answering all kinds of questions automatically. But all of the experiments so far have been \"safe\". The datasets and techniques were pre-selected and had predictable results. But now it is time to journey into the wild. The final project is described below, but first, we should talk about some of the non-technical things that can go wrong with bringing machine learning into the real world.\n",
    "\n",
    "## 1. Dataset Overfitting\n",
    "One of the biggest ways that machine learning can go horribly wrong is when the deployment environment is significantly different than the training environment. Some classic examples:\n",
    "- Self-driving car data was predominantly collected during the day with favorable weather conditions. Night-time snow could lead to fatal accidents.\n",
    "- Training images for classification were collected during different seasons, leading to spurious correlations with background features\n",
    "\n",
    "## 2. Leaked Labels\n",
    "Supervised training data can accidentally provide information that is not normally available at test-time. A common mistake is including future data in a sequence prediction/forecasting task. (e.g., Instead of predicting $t_n$ from $\\{t_1, ..., t_{n-1}\\}$, the algorithm has been trained to solve the trivial problem of $t_n$ from $\\{t_1, ..., t_{n}\\}$.)\n",
    "\n",
    "## 3. Non-Stationary Data/Non-Stationary Problems\n",
    "When forces outside the scope of the model alter the distribution or outputs of different input data, performance can be worse than expected. For example:\n",
    "- A model is trained to predict ocean temperature based on depth, latitude, longitude, and day of the year, but the model doesn't account for global climate change over multiple years.\n",
    "- Diseases occur in a diagnostic dataset with roughly equal probability (not representative of the natural distribution), causing anthrax to be diagnosed on the evidence of a runny nose.\n",
    "- All diagnostic data are for patients with known diseases, so no patient is ever diagnosed as having nothing wrong with them.\n",
    "\n",
    "## 4. Feedback Effects\n",
    "Sometimes the trained models cause a shift in the input data they collect. For example, prediction of interesting phenomena (say, the presence of whales) causes deployment of additional resources (say, boats and drones), in turn affecting the presence of the phenomenon (say, driving the whales away). These effects are generally captured by reinforcement learning problems, but many are unfortunately very big, difficult to model, and not easily solved by existing techniques. This means the best tools for prevention are common sense and vigilance.\n",
    "\n",
    "## 5. Wrong Objectives/Metrics\n",
    "One of the subtlest problems with deployment of machine learning is measuring the wrong objective. This can show up in a lot of devious ways:\n",
    "- Minimizing mean squared error doesn't model bimodal distributions well.\n",
    "- Data reconstruction may not be caputring the task-relevant features.\n",
    "- Optimizing to minimize cost may not account for safety or environmental impact.\n",
    "- Minimizing Euclidean distance assumes features have equal weight and are translation-invariant.\n",
    "- Perhaps not all cases/training points should have equal weight.\n",
    "- Perhaps not all forms of error should have equal weight.\n",
    "- Reinforcement Learning can game the system\n",
    "\n",
    "[Credit to OpenAI for the GIF](https://openai.com/blog/faulty-reward-functions/)\n",
    "\n",
    "![RL Fail Video](./rl_fail.gif \"Poorly Specified Rewards lead to Bad Behavior\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "1. Pick a Dataset/Problem.\n",
    "2. Ask a question that can be answered using Machine Learning (prediction, interpolation, interpretation, decision, etc.).\n",
    "3. Identify the structure of the inputs and outputs, and choose an evaluation metric.\n",
    "4. Apply a baseline (e.g. random guessing, or always outputting the same value) and measure performance.\n",
    "5. Apply one of the techniques from the course, or a combination.\n",
    "6. Visualize the model performance, model output, or model application.\n",
    "7. Summarize (and present) your findings (5 minute presentation).\n",
    "\n",
    "Summary: Find Data, Pick Model, Do Science\n",
    "\n",
    "# Stretch Goals:\n",
    "- Test Multiple Techniques/Models and Compare Performance.\n",
    "- Explore Failure Cases of a Trained Model.\n",
    "- Look for Cutting-Edge techniques for related problems, or test some modifications to improve performance.\n",
    "- Export Outputs for use by others, or provide an API to the trained model. (Serialize either the outputs or the model.)\n",
    "- Explore Additional Problems that can be answered with the same data, or identify shortcomings of the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
